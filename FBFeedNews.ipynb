{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adem\n",
      "succesfully created C:\\Users\\Adem/FBDirectory/\n",
      "succesfully created C:\\Users\\Adem/FBFinalDirectory/\n",
      "created\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "c\n",
      "updated\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "c\n",
      "updated\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "c\n",
      "updated\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "c\n",
      "updated\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "c\n",
      "updated\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "c\n",
      "updated\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "c\n",
      "updated\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "c\n",
      "updated\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "c\n",
      "updated\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "c\n",
      "updated\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "no updates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job \"update (trigger: interval[0:01:00], next run at: 2020-12-02 20:48:33 CET)\" raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 159, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\", line 80, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\", line 70, in create_connection\n",
      "    sock.connect(sa)\n",
      "OSError: [WinError 10065] Une opération a été tentée sur un hôte impossible à atteindre\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 343, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 839, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 301, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 168, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e)\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x0000024DC07BA108>: Failed to establish a new connection: [WinError 10065] Une opération a été tentée sur un hôte impossible à atteindre\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 399, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='seekingalpha.com', port=443): Max retries exceeded with url: /feed.xml (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024DC07BA108>: Failed to establish a new connection: [WinError 10065] Une opération a été tentée sur un hôte impossible à atteindre'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\apscheduler\\executors\\base.py\", line 125, in run_job\n",
      "    retval = job.func(*job.args, **job.kwargs)\n",
      "  File \"<ipython-input-1-eda25ab852cb>\", line 39, in update\n",
      "    response = requests.get(URL)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 60, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='seekingalpha.com', port=443): Max retries exceeded with url: /feed.xml (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024DC07BA108>: Failed to establish a new connection: [WinError 10065] Une opération a été tentée sur un hôte impossible à atteindre'))\n",
      "Run time of job \"update (trigger: interval[0:01:00], next run at: 2020-12-02 22:30:33 CET)\" was missed by 0:00:39.837176\n",
      "Job \"update (trigger: interval[0:01:00], next run at: 2020-12-02 22:31:33 CET)\" raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 159, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\", line 57, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\socket.py\", line 748, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 343, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 839, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 301, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 168, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e)\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x0000024DBE4EC308>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 399, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='seekingalpha.com', port=443): Max retries exceeded with url: /feed.xml (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024DBE4EC308>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\apscheduler\\executors\\base.py\", line 125, in run_job\n",
      "    retval = job.func(*job.args, **job.kwargs)\n",
      "  File \"<ipython-input-1-eda25ab852cb>\", line 39, in update\n",
      "    response = requests.get(URL)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 60, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\Adem\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='seekingalpha.com', port=443): Max retries exceeded with url: /feed.xml (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024DBE4EC308>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "updated\n",
      "no updates\n",
      "no updates\n",
      "no updates\n",
      "c\n",
      "updated\n",
      "no updates\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-eda25ab852cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlockingScheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'interval'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminutes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\apscheduler\\schedulers\\blocking.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEvent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBlockingScheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_main_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshutdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\apscheduler\\schedulers\\blocking.py\u001b[0m in \u001b[0;36m_main_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mwait_seconds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTIMEOUT_MAX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mSTATE_STOPPED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait_seconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mwait_seconds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from xml.etree.ElementTree import parse\n",
    "import pytz \n",
    "import os\n",
    "#update\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "#GenericDirectories\n",
    "from pathlib import Path\n",
    "\n",
    "home_directory = str(Path.home())\n",
    "print(home_directory)\n",
    "DownloadedFilesDirectory=home_directory+'/FBDirectory/'\n",
    "ExportedFilesDirectory=home_directory+'/FBFinalDirectory/'\n",
    "# define the name of the directory to be created\n",
    "Directories=[DownloadedFilesDirectory,ExportedFilesDirectory]\n",
    "for d in Directories:\n",
    "    if not(os.path.exists(d)):\n",
    "        try:\n",
    "            os.mkdir(d)\n",
    "            print(\"succesfully created \"+d)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory failed \" +d)\n",
    "    else:\n",
    "        print (\"Directory exists already \" +d)\n",
    "def update():\n",
    "    def _get_fb_id(row):\n",
    "        '''\n",
    "        Private util method to construct fb_id (fusionbase id) by hashing row values.\n",
    "        '''\n",
    "        concat_str = ''.join([str(x) for x in np.array(row)])\n",
    "\n",
    "        return hashlib.sha3_256(concat_str.encode('utf-8')).hexdigest()[:32]\n",
    "\n",
    "    URL = \"https://seekingalpha.com/feed.xml\"\n",
    "    response = requests.get(URL)\n",
    "\n",
    "    with open(DownloadedFilesDirectory+'feed.xml', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    document = parse(DownloadedFilesDirectory+'feed.xml')\n",
    "\n",
    "    title=[]\n",
    "    guid=[]\n",
    "    link=[]\n",
    "    categS=[]\n",
    "    categA=[]\n",
    "    pubDate=[]\n",
    "    #collecting data from the Xml file and save it in lists\n",
    "    for item in document.iterfind('channel/item'):\n",
    "        title.append(item.findtext('title'))\n",
    "        guid.append(item.findtext('guid'))\n",
    "        link.append(item.findtext('link'))\n",
    "        symb = [x.text for x in item.findall('category/[@type=\"symbol\"]')]\n",
    "        categS.append(str(symb))\n",
    "        aut = [x.text for x in item.findall('category/[@type=\"author\"]')]\n",
    "        categA.append(str(aut))\n",
    "        pubDate.append(item.findtext('pubDate'))\n",
    "    #creating dataframe and filling it with lists content    \n",
    "    df = pd.DataFrame({'title': title, 'guid':guid, 'link':link,'stock_symbol':categS,'stock_author':categA,'publication_datetime':pubDate})\n",
    "    #transforming data for some columns(stock_symbol,stock_author)\n",
    "    df['stock_symbol'] = df['stock_symbol'].str.replace(r'\\'', '').str.lstrip(\"[\").str.rstrip(\"]\")\n",
    "    df['stock_author'] = df['stock_author'].str.replace(r'\\'', '').str.lstrip(\"[\").str.rstrip(\"]\")\n",
    "    df['publication_datetime']=pd.to_datetime(df.publication_datetime,utc=True)\n",
    "    #adding fusionBase Id and datetime\n",
    "    fbId=[]\n",
    "    fbId =  np.array([_get_fb_id(row) for row in df.values])\n",
    "    df.set_index(fbId,inplace=True)\n",
    "    df.index.set_names('fb_id',inplace=True)\n",
    "    dateTime=[]\n",
    "    for i in range(len(df)):\n",
    "            dateTime.append(datetime.utcnow())\n",
    "    df['fb_datetime']=dateTime\n",
    "    dftemp=df\n",
    "    if not(os.path.exists(ExportedFilesDirectory+'feed.parquet.gzip')):\n",
    "            #Creating a new parquet file and filling it with data recently extracted\n",
    "            dftemp.to_parquet(ExportedFilesDirectory+'feed.parquet.gzip',compression='gzip',engine='fastparquet',index=True)\n",
    "            print('created')\n",
    "\n",
    "    else:\n",
    "\n",
    "            #extracting data from the existing parquet file \n",
    "            df=pd.read_parquet(ExportedFilesDirectory+'feed.parquet.gzip',engine='fastparquet')\n",
    "            #Checking for new updates\n",
    "            if dftemp.iloc[0,5]!=df.iloc[0,5]:\n",
    "                #Updating parquet file\n",
    "                df=pd.concat([dftemp,df],axis=0)\n",
    "                df.drop_duplicates(subset='publication_datetime',keep='last')\n",
    "                df.to_parquet(ExportedFilesDirectory+'feed.parquet.gzip',compression='gzip',engine='fastparquet',index=True)\n",
    "                print('updated')\n",
    "            else:\n",
    "                #No updates\n",
    "                print('no updates')\n",
    "scheduler = BlockingScheduler()\n",
    "scheduler.add_job(update, 'interval', minutes=1)\n",
    "scheduler.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
